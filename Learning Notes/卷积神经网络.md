

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [卷积神经网络（Convolution Neural Network,CNN）](#卷积神经网络convolution-neural-networkcnn)
	* [基础概念](#基础概念)
		* [基础概念](#基础概念-1)
			* [填充（padding）](#填充padding)
			* [步长（stride）](#步长stride)
			* [池化（pooling）](#池化pooling)
				* [最大池化（max pooling）](#最大池化max-pooling)
				* [平均池化（average pooling）](#平均池化average-pooling)
			* [激活函数（Activate Function）](#激活函数activate-function)
			* [标准化（normalization）](#标准化normalization)
				* [批标准化（batch normalization）](#批标准化batch-normalization)
		* [感受野（Receptive field，RF）](#感受野receptive-fieldrf)
			* [感受野可视化](#感受野可视化)
			* [感受野的计算](#感受野的计算)
		* [权值共享](#权值共享)
		* [卷积层](#卷积层)
		* [下采样](#下采样)
		* [全连接（Full connect,FC）](#全连接full-connectfc)
		* [SoftMax](#softmax)
	* [经典CNN模型](#经典cnn模型)
		* [总览](#总览)
		* [LeNet](#lenet)
		* [AlexNet](#alexnet)
		* [ZFNet](#zfnet)
		* [GoogleNet-inception](#googlenet-inception)
		* [VGG](#vgg)
		* [ResNet](#resnet)
		* [DenseNet](#densenet)
		* [U-Net](#u-net)
		* [PointNet](#pointnet)
		* [PointNet++](#pointnet-1)
	* [注意事项](#注意事项)

<!-- /code_chunk_output -->






# 卷积神经网络（Convolution Neural Network,CNN）

    性质：位移、尺度、缩放、非线性形变稳定性

卷积神经网络（CNN）是一类人工神经网络。因此，它们是由被称为「神经元」的单元构成的，这些单元可根据输入的加权和输出一个活动水平。这个活动水平通常是输入的非线性函数，通常只是一个整流线性单元（ReLU），其中当输入全为正时活动等于输入，当输入全为非正时活动等于 0。

CNN 的独特之处是神经元之间的连接的构建方式。在一个前馈神经网络中，单元会被组织成层的形式，给定层的单元只会获得来自其下面一层的输入（即不会有来自同一层或后续层的其它单元的输入，大多数情况下也不会有来自之前超过 1 层的输入）。CNN 是**前馈网络**。但不同于标准的单纯的前馈网络，CNN 中的单元具有一种空间排列。在每一层，单元都会被组织成 2D 网格形式，这被称为特征图（feature map）。每一个特征图都是在其下面一层上执行卷积所得的结果（CNN 也因此得名）。这意味着在其下面一层的每个位置都应用了同样的卷积过滤器（权重集）。因此，在该 2D 网格上特定位置的单元只能收到来自其下面一层相似位置的单元的输入。此外，输入上附带的权重对一个特征图中的每个单元都是一样的（而各个特征图各不相同）。
<div align="center">
<img src=CNN_stuffs/CNN-1.jpg alt=CNN>  

Convolution Neural Network
</div>


大多数现代 CNN 都有多个（至少 5）这样的层，其中最后一层会向一个全连接层馈送数据。全连接层就像是标准的前馈网络，其中没有空间布局或受限的连接。通常会有 2-3 个全连接层连在一起使用，并且网络的最后一层执行分类。举个例子，如果该网络执行的是 10 类目标分类，那么最后一层将会有 10 个单元，会有一个 softmax 操作应用在它们的活动水平上以得到每个类别相关的概率。

这些网络主要通过监督学习和反向传播训练。这时，提供给网络的输入是图像及其相关类别标签构成的配对集。图像像素值输入网络的第一层，然后网络最后一层得出一个预测类别。如果这个预测得到的标签与所提供的标签不一致，那么就会计算梯度，确定应该如何修改权重（即卷积过滤器中的值）以使分类正确。如此重复很多很多次（很多网络都是在 ImageNet 数据库上训练的，这个数据库包含 1000 个目标类别的超过 100 万张图像），就能得到在留存测试图像上有很高准确度的模型。CNN 的某些变体模型现在已能达到 4.94% 乃至更低的错误率，优于人类水平。要得到优良的表现，通常需要很多训练「技巧」，比如智能学习率选择和权重正则化（主要是通过 dropout，即在每个训练阶段都有随机一半的权重关闭）。

历史上曾使用无监督预训练来初始化权重，然后再使用监督学习来进行改善。但是，这似乎已经不再是优越性能所必需的了。

*参考：* [卷积神经网络十五问](http://baijiahao.baidu.com/s?id=1602962974655940776&wfr=spider&for=pc)；[卷积神经网络超详细介绍](https://blog.csdn.net/jiaoyangwm/article/details/80011656#1_2)

## 基础概念

### 基础概念
#### 填充（padding）
#### 步长（stride）
#### 池化（pooling）
##### 最大池化（max pooling）
##### 平均池化（average pooling）
#### 激活函数（Activate Function）
#### 标准化（normalization）
##### 批标准化（batch normalization）



### 感受野（Receptive field，RF）

    感受野 指的是一个特定的CNN特征（特征图上的某个点）在输入空间所受影响的区域。
    一个感受野可以用中心位置(center location)和大小(size)来表征。

某一层的卷积核大小对应于在上一层输出的“图像”上的“视野”大小。比如，某层有 3x3 的卷积核，那就是一个 3x3 大小的滑动窗口在该层的输入“图像”上去扫描，我们就可以谈相对于上一层，说该层下特征图（feature map）当中任一特征点（feature）的“感受野”大小只有 3x3（打引号说明术语引用不够严谨）。

对于一个CNN特征来说，感受野中的每个像素值（pixel）并不是同等重要。一个像素点越接近感受野中心，它对输出特征的计算所起作用越大。这意味着某一个特征不仅仅是受限在输入图片中某个特定的区域（感受野），并且呈指数级聚焦在区域的中心。

#### 感受野可视化
假定我们所考虑的 CNN 架构是对称的，并且输入图像也是方形的。这样的话，我们就忽略掉不同长宽所造成的维度不同。

Way1 对应为通常的一种理解感受野的方式。在下方左侧的上图中，是在 5x5 的图像(蓝色)上做一个 3x3 卷积核的卷积计算操作，步长为2，padding 为1，所以输出为 3x3 的特征图(绿色)。那么该特征图上的每个特征(1x1)对应的感受野，就是 3x3。在下方左侧的下图中，是在上述基础上再加了一个完全一样的卷积层。对于经过第二层卷积后其上的一个特征(如红色圈)在上一层特征图上“感受”到 3x3 大小，该 3x3 大小的每个特征再映射回到图像上，就会发现由 7x7 个像素点与之关联，有所贡献。于是，就可以说第二层卷积后的特征其感受野大小是 7x7（需要自己画个图，好好数一数）。Way2 （下方右侧的图像）是另一种理解的方式，主要的区别仅仅是将两层特征图上的特征不进行“合成”，而是保留其在前一层因“步长”而产生的影响。
<div align="center">
<img src=CNN_stuffs\RF-1.png alt=RF1>
</div>
Way2 的理解方式其实更具有一般性，我们可以无需考虑输入图像的大小对感受野进行计算。如下图：
<div align="center">
<img src=CNN_stuffs\RF-2.png alt=RF2>
</div>
虽然，图上绘制了输入 9x9 的图像（蓝色），但是它的大小情况是无关紧要的，因为我们现在只关注某“无限”大小图像某一像素点为中心的一块区域进行卷积操作。首先，经过一个 3x3 的卷积层（padding=1，stride=2）后，可以得到特征输出（深绿色）部分。其中深绿色的特征分别表示卷积核扫过输入图像时，卷积核中心点所在的相对位置。此时，每个深绿色特征的感受野是 3x3 （浅绿）。这很好理解，每一个绿色特征值的贡献来源是其周围一个 3x3 面积。再叠加一个 3x3 的卷积层（padding=1，stride=2）后，输出得到 3x3 的特征输出（橙色）。此时的中心点的感受野所对应的是黄色区域 7x7，代表的是输入图像在中心点橙色特征所做的贡献。

    也就是说两层 3x3 的卷积层直接堆叠后（无池化）可以算的有感受野是 5x5，三层堆叠后的感受野就是 7x7。

#### 感受野的计算
为了计算CNN每一层的感受野，除了要知道特征图每个维度的特征数$n$，还需要记录每一层的其他信息，这包括当前层的感受野大小$r$，两个相邻特征的距离（跳跃的距离，如前面可视化所示）$j$，和左上角特征（第一个特征）的中心坐标$start$。注意感受野（其实是特征图第一个特征的感受野）的中心坐标就等于这个特征的中心坐标，就如前面可视化中所示。当采用的卷积其核大小为$k$，$padding$大小为$p$，步长为$s$，输出特征图的感受野可以按照如下公式计算：
$$\begin{array}{l}
    n_{out}=\left[\frac{n_{in}+2p-k}{s}\right]+1\\
    j_{out}=j_{in}\times s\\
    r_{out}=r_{in}+\left(k-1\right)\times j_{in}\\
    start_{out}=start_{in}+\left(\frac{k-1}{2}-p\right)\times j_{in}
\end{array}$$
- 第一个式子根据输入特征图大小以及卷积参数计算输出特征图大小;
- 第二个式子计算输出特征图的特征间的间隔j，其等于上一层的间隔值乘以卷积的步长，所以间隔值将是按照步长呈指数级增长;
- 第三个式子计算输出特征图的感受野大小，其等于前一层感受野大小加上$(k−1)∗j_{in}$，所以感受野是呈指数级增加，并且还有一个因子$k−1$;
- 第四个式子计算输出特征图的第一个特征感受野的中心坐标，其等于第一层的中心坐标加上$(k−1)/2∗j_{in}$，再减去$p∗j_{in}$，注意两项都要乘以前一层的间隔距离以得到实际距离。
<div align="center">
<img src=CNN_stuffs\RF-3.png alt=RF3>
</div>
上图中除了公式和说明部分外，有两行分别代表的是第一层卷积和第二层卷积。在每行中，应从左往右观察卷积核计算和操作。

第一层比较简单，最后输出 3x3 绿色的特征图，每个特征有阴影框大小来表示每个特征对应的感受野大小 3x3。其中$start_1$表示的 0.5 几何半径，我已经用红色标识出来，对应于阴影面积覆盖到的绿色面积的几何半径。

第二层，由于有一个单位的 padding，所以 3x3 卷积核是按照蓝色箭头标记作为的起始方向开始，在所有的绿色位置上挪动的。最后算得特征的感受野大小为 7x7，亦对应于阴影框和阴影区域部分。其中$start_2$是 0.5 也已经用红色标记了出来。

对于感受野大小的计算，另外有一个博客（[Calculating Receptive Field of CNN](http://shawnleezx.github.io/blog/2017/02/11/calculating-receptive-field-of-cnn/)）给出一个更简洁的计算公式，对于第k层的感受野大小计算如下： 

$$l_k = l_{k-1} + ((f_k - 1) * \prod_{i=1}^{k-1}s_i)$$

其中$l_k−1$是第$k−1$层的感受野大小，而$f_k$是当前层的卷积核大小，$s_i$是第$i$层的步长。从这个公式可以看到，相比前一层，当前层的感受野大小在两层之间增加了$(f_k - 1) * \prod_{i=1}^{k-1}s_i$ ，如果$stride$大于1的话，这是一个指数级增加。这个公式也可以这样理解，对于第 $k$ 层，其卷积核为 $f_k$，那么相比前一层需要计算 $f_k$ 个位置（或者神经元，意思是 k 层的一个位置在 $k−1$ 层的视野大小是 $f_k$ ），但是这些位置要一直向前扩展到输入层。对于第一个位置，扩展后的感受野为 $l_k−1$ ，正好是前一层的感受野大小，但是对于剩余的 $f_k−1$ 个位置就要看stride大小，你需要扩展到前面所有层的$stride$（注意不包括当前层的$stride$，当前层的$stride$只会影响后面层的感受野），所以需要乘以 $\prod_{i=1}^{k-1}s_i$ ，这样剩余 $f_k−1$ 个位置的感受野大小就是 $(f_k - 1) * \prod_{i=1}^{k-1}s_i$ ，和第一个位置的感受野加到一起就是上面的公式了。。其实这个公式算是整合了前面的公式2和公式3（第一层的$j=1$），两个本质上是一致的，不过如果你仅想计算感受野大小可以用这个公式更方便。

参考：[关于感受野 (Receptive field) 你该知道的事](https://iphysresearch.github.io/posts/receptive_field.html)； [你知道如何计算CNN感受野吗？这里有一份详细指南](https://blog.csdn.net/xiaohu2022/article/details/80647180)
### 权值共享
### 卷积层
### 下采样

### 全连接（Full connect,FC）
### SoftMax

## 经典CNN模型
### 总览
| 模型名称 | 发布时间 | 特性  | 其他  |
| :------: | :------: | :---: | :---: |
|          |          |       |       |
### LeNet
### AlexNet
### ZFNet
### GoogleNet-inception
### VGG
### ResNet
### DenseNet
### U-Net
### PointNet
### PointNet++

## 注意事项